{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import requests, StringIO\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "NUM_CLASSES = 200\n",
    "NUM_IMAGES_PER_CLASS = 500\n",
    "NUM_IMAGES = NUM_CLASSES * NUM_IMAGES_PER_CLASS\n",
    "TRAINING_IMAGES_DIR = '/datasets/tmp/cg181fdn/tiny-imagenet-200/train/'\n",
    "TRAIN_SIZE = NUM_IMAGES\n",
    "\n",
    "NUM_VAL_IMAGES = 10000\n",
    "VAL_IMAGES_DIR = '/datasets/tmp/cg181fdn/tiny-imagenet-200/val/'\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "NUM_CHANNELS = 3\n",
    "IMAGE_ARR_SIZE = IMAGE_SIZE * IMAGE_SIZE * NUM_CHANNELS\n",
    "IMAGES_URL = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n",
    "\n",
    "def download_images(url):\n",
    "    if (os.path.isdir(TRAINING_IMAGES_DIR)):\n",
    "        print ('Images already downloaded...')\n",
    "        return\n",
    "    r = requests.get(url, stream=True)\n",
    "    print ('Downloading ' + url )\n",
    "    zip_ref = zipfile.ZipFile(StringIO.StringIO(r.content))\n",
    "    zip_ref.extractall('/datasets/tmp/cg181fdn/')\n",
    "    zip_ref.close()\n",
    "\n",
    "def load_training_images(image_dir, batch_size=500):\n",
    "\n",
    "    image_index = 0\n",
    "    \n",
    "    images = np.ndarray(shape=(NUM_IMAGES, IMAGE_ARR_SIZE))\n",
    "    names = []\n",
    "    labels = []                       \n",
    "    \n",
    "    # Loop through all the types directories\n",
    "    for type in os.listdir(image_dir):\n",
    "        if os.path.isdir(image_dir + type + '/images/'):\n",
    "            type_images = os.listdir(image_dir + type + '/images/')\n",
    "            # Loop through all the images of a type directory\n",
    "            batch_index = 0;\n",
    "            #print (\"Loading Class \", type)\n",
    "            for image in type_images:\n",
    "                image_file = os.path.join(image_dir, type + '/images/', image)\n",
    "\n",
    "                # reading the images as they are; no normalization, no color editing\n",
    "                image_data = mpimg.imread(image_file) \n",
    "                #print ('Loaded Image', image_file, image_data.shape)\n",
    "                if (image_data.shape == (IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)):\n",
    "                    images[image_index, :] = image_data.flatten()\n",
    "\n",
    "                    labels.append(type)\n",
    "                    names.append(image)\n",
    "                    \n",
    "                    image_index += 1\n",
    "                    batch_index += 1\n",
    "                if (batch_index >= batch_size):\n",
    "                    break;\n",
    "                    \n",
    "    return (images, np.asarray(labels), np.asarray(names))\n",
    "\n",
    "def get_label_from_name(data, name):\n",
    "    for idx, row in data.iterrows():       \n",
    "        if (row['File'] == name):\n",
    "            return row['Class']\n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "def load_validation_images(testdir, validation_data, batch_size=NUM_VAL_IMAGES):\n",
    "    labels = []\n",
    "    names = []\n",
    "    image_index = 0\n",
    "    \n",
    "    images = np.ndarray(shape=(batch_size, IMAGE_ARR_SIZE))\n",
    "    val_images = os.listdir(testdir + '/images/')\n",
    "           \n",
    "    # Loop through all the images of a val directory\n",
    "    batch_index = 0;\n",
    "    \n",
    "    \n",
    "    for image in val_images:\n",
    "        image_file = os.path.join(testdir, 'images/', image)\n",
    "        #print (testdir, image_file)\n",
    "\n",
    "        # reading the images as they are; no normalization, no color editing\n",
    "        image_data = mpimg.imread(image_file) \n",
    "        if (image_data.shape == (IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)):\n",
    "            images[image_index, :] = image_data.flatten()\n",
    "            image_index += 1\n",
    "            labels.append(get_label_from_name(validation_data, image))\n",
    "            names.append(image)\n",
    "            batch_index += 1\n",
    "            \n",
    "        if (batch_index >= batch_size):\n",
    "            break;\n",
    "    \n",
    "    print (\"Loaded Validation images \", image_index)\n",
    "    return (images, np.asarray(labels), np.asarray(names))\n",
    "   \n",
    "    \n",
    "\n",
    "def plot_object(data):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    image = data.reshape(IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n",
    "    plt.imshow(image, cmap = matplotlib.cm.binary,\n",
    "               interpolation=\"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_objects(instances, images_per_row=10, **options):\n",
    "    size = IMAGE_SIZE\n",
    "    images_per_row = min(len(instances), images_per_row)\n",
    "    images = [instance.reshape(size,size,NUM_CHANNELS) for instance in instances]\n",
    "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
    "    row_images = []\n",
    "    n_empty = n_rows * images_per_row - len(instances)\n",
    "    images.append(np.zeros((size, size * n_empty)))\n",
    "    for row in range(n_rows):\n",
    "        if (row == len(instances)/images_per_row):\n",
    "            break\n",
    "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
    "        row_images.append(np.concatenate(rimages, axis=1))\n",
    "    image = np.concatenate(row_images, axis=0)\n",
    "    plt.imshow(image, **options)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def get_next_batch(batchsize=50):\n",
    "    for cursor in range(0, len(training_images), batchsize):\n",
    "        batch = []\n",
    "        batch.append(training_images[cursor:cursor+batchsize])\n",
    "        batch.append(training_labels[cursor:cursor+batchsize])       \n",
    "        yield batch\n",
    "        \n",
    "def get_val_batch(batchsize=50):\n",
    "    for cursor in range(0, len(val_images), batchsize):\n",
    "        batch = []\n",
    "        batch.append(val_images[cursor:cursor+batchsize])\n",
    "        batch.append(val_labels[cursor:cursor+batchsize])\n",
    "        yield batch\n",
    "\n",
    "def get_next_labels(batchsize=50):\n",
    "    for cursor in range(0, len(training_images), batchsize):\n",
    "        yield training_labels[cursor:cursor+batchsize]  \n",
    "    \n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://cs231n.stanford.edu/tiny-imagenet-200.zip\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import cPickle\n",
    "\n",
    "def load_zipped_pickle(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        loaded_object = cPickle.load(f)\n",
    "        return loaded_object\n",
    "\n",
    "download_images(IMAGES_URL)\n",
    "# training_images, training_labels, training_files = load_training_images(TRAINING_IMAGES_DIR, batch_size=500)\n",
    "\n",
    "# shuffle_index = np.random.permutation(len(training_labels))\n",
    "# training_images = training_images[shuffle_index]\n",
    "# training_labels = training_labels[shuffle_index]\n",
    "# training_files  = training_files[shuffle_index]\n",
    "\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# training_le = le.fit(training_labels)\n",
    "# training_labels_encoded = training_le.transform(training_labels)\n",
    "\n",
    "# print (\"First 30 Training Labels\", training_labels_encoded[0:30])\n",
    "# plot_objects(training_images[0:30])\n",
    "\n",
    "\n",
    "# val_data = pd.read_csv(VAL_IMAGES_DIR + 'val_annotations.txt', sep='\\t', header=None, names=['File', 'Class', 'X', 'Y', 'H', 'W'])\n",
    "# val_images, val_labels, val_files = load_validation_images(VAL_IMAGES_DIR, val_data, batch_size=50)\n",
    "# #val_images, val_labels, val_files = load_validation_images(VAL_IMAGES_DIR, val_data)\n",
    "# val_labels_encoded = training_le.transform(val_labels)\n",
    "# plot_objects(val_images[0:30])\n",
    "# print (val_labels_encoded[0:30])\n",
    "\n",
    "# data = load_zipped_pickle(\"tinyImageData\")\n",
    "\n",
    "# shuffle_train_index = np.random.permutation(len(data['train']['data']))\n",
    "# training_images = np.array(data['train']['data'])[shuffle_train_index]\n",
    "# training_labels = np.array(data['train']['target'])[shuffle_train_index]\n",
    "\n",
    "# shuffle_val_index = np.random.permutation(len(data['val']['data']))\n",
    "# val_images = np.array(data['val']['data'])[shuffle_val_index]\n",
    "# val_labels = np.array(data['val']['target'])[shuffle_val_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height = IMAGE_SIZE\n",
    "width = IMAGE_SIZE\n",
    "channels = NUM_CHANNELS\n",
    "n_inputs = height * width * channels\n",
    "n_outputs = 200\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])\n",
    "y = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "\n",
    "#input shape [-1, 64, 64, 3]\n",
    "conv1 = tf.layers.conv2d(\n",
    "            inputs=X_reshaped, \n",
    "            filters=64, \n",
    "            kernel_size=[11,11],\n",
    "            padding='SAME',\n",
    "            activation=tf.nn.relu, \n",
    "            name=\"conv1\")\n",
    "\n",
    "bn1 = tf.layers.batch_normalization(\n",
    "            inputs=conv1, \n",
    "            name=\"bn1\")\n",
    "\n",
    "pool1 = tf.layers.max_pooling2d(\n",
    "            inputs=bn1,\n",
    "            pool_size=[2, 2],\n",
    "            strides=2,\n",
    "            name=\"pool1\")\n",
    "\n",
    "conv2 = tf.layers.conv2d(\n",
    "            inputs=pool1, \n",
    "            filters=128, \n",
    "            kernel_size=[7,7],\n",
    "            padding='SAME',\n",
    "            activation=tf.nn.relu, \n",
    "            name=\"conv2\")\n",
    "\n",
    "bn2 = tf.layers.batch_normalization(\n",
    "            inputs=conv2, \n",
    "            name=\"bn2\")\n",
    "\n",
    "pool2 = tf.layers.max_pooling2d(\n",
    "            inputs=bn2,\n",
    "            pool_size=[2, 2],\n",
    "            strides=2,\n",
    "            name=\"pool2\")\n",
    "\n",
    "conv3 = tf.layers.conv2d(\n",
    "            inputs=pool2, \n",
    "            filters=192, \n",
    "            kernel_size=[3,3],\n",
    "            padding='SAME',\n",
    "            activation=tf.nn.relu, \n",
    "            name=\"conv3\")\n",
    "\n",
    "bn3 = tf.layers.batch_normalization(\n",
    "            inputs=conv3, \n",
    "            name=\"bn3\")\n",
    "\n",
    "pool3 = tf.layers.max_pooling2d(\n",
    "            inputs=bn3,\n",
    "            pool_size=[2, 2],\n",
    "            strides=2,\n",
    "            name=\"pool3\")\n",
    "\n",
    "conv4 = tf.layers.conv2d(\n",
    "            inputs=pool3, \n",
    "            filters=256, \n",
    "            kernel_size=[3,3],\n",
    "            padding='SAME',\n",
    "            activation=tf.nn.relu, \n",
    "            name=\"conv4\")\n",
    "\n",
    "bn4 = tf.layers.batch_normalization(\n",
    "            inputs=conv4, \n",
    "            name=\"bn4\")\n",
    "\n",
    "pool4 = tf.layers.max_pooling2d(\n",
    "            inputs=bn4,\n",
    "            pool_size=[2, 2],\n",
    "            strides=2,\n",
    "            name=\"pool4\")\n",
    "\n",
    "#pool2_flat = tf.reshape(pool1, [-1, 8 * 8 * 64])\n",
    "# Dense Layer\n",
    "pool4_flat = tf.contrib.layers.flatten(pool4)\n",
    "\n",
    "dense1 = tf.layers.dense(inputs=pool4_flat, units=4096, activation=tf.nn.relu)\n",
    "bn5 = tf.layers.batch_normalization(\n",
    "            inputs=dense1, \n",
    "            name=\"bn5\")\n",
    "\n",
    "dense2 = tf.layers.dense(inputs=bn5, units=512, activation=tf.nn.relu)\n",
    "bn6 = tf.layers.batch_normalization(\n",
    "            inputs=dense2, \n",
    "            name=\"bn6\")\n",
    "\n",
    "# Logits Layer\n",
    "logits = tf.layers.dense(inputs=bn6, units=200, name='output')\n",
    "Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0001)\n",
    "#optimizer = tf.keras.optimizers.SGD(lr=0.001, momentum=0.9, decay=0.000001, nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Batch finished: 1000, Accuracy: 0.005000, Loss: 22.481092\n",
      "Total Batch finished: 2000, Accuracy: 0.003500, Loss: 20.707058\n",
      "Total Batch finished: 3000, Accuracy: 0.004000, Loss: 18.543517\n",
      "Total Batch finished: 4000, Accuracy: 0.004750, Loss: 16.815016\n",
      "Total Batch finished: 5000, Accuracy: 0.005400, Loss: 15.306990\n",
      "Total Batch finished: 6000, Accuracy: 0.004667, Loss: 14.179837\n",
      "Total Batch finished: 7000, Accuracy: 0.004571, Loss: 13.310828\n",
      "Total Batch finished: 8000, Accuracy: 0.005250, Loss: 12.632472\n",
      "Total Batch finished: 9000, Accuracy: 0.005556, Loss: 12.074749\n",
      "Total Batch finished: 10000, Accuracy: 0.005900, Loss: 11.595311\n",
      "Total Batch finished: 11000, Accuracy: 0.006273, Loss: 11.203014\n",
      "Total Batch finished: 12000, Accuracy: 0.006250, Loss: 10.852432\n",
      "Total Batch finished: 13000, Accuracy: 0.006000, Loss: 10.557579\n",
      "Total Batch finished: 14000, Accuracy: 0.005929, Loss: 10.293337\n",
      "Total Batch finished: 15000, Accuracy: 0.005867, Loss: 10.062110\n",
      "Total Batch finished: 16000, Accuracy: 0.005875, Loss: 9.846153\n",
      "Total Batch finished: 17000, Accuracy: 0.006118, Loss: 9.646032\n",
      "Total Batch finished: 18000, Accuracy: 0.006000, Loss: 9.475522\n",
      "Total Batch finished: 19000, Accuracy: 0.006000, Loss: 9.309819\n",
      "Total Batch finished: 20000, Accuracy: 0.005850, Loss: 9.158735\n",
      "Total Batch finished: 21000, Accuracy: 0.005905, Loss: 9.027520\n",
      "Total Batch finished: 22000, Accuracy: 0.005955, Loss: 8.901146\n",
      "Total Batch finished: 23000, Accuracy: 0.005913, Loss: 8.780914\n",
      "Total Batch finished: 24000, Accuracy: 0.005792, Loss: 8.673239\n",
      "Total Batch finished: 25000, Accuracy: 0.005800, Loss: 8.572958\n",
      "Total Batch finished: 26000, Accuracy: 0.005731, Loss: 8.476819\n",
      "Total Batch finished: 27000, Accuracy: 0.005667, Loss: 8.388698\n",
      "Total Batch finished: 28000, Accuracy: 0.005643, Loss: 8.306512\n",
      "Total Batch finished: 29000, Accuracy: 0.005586, Loss: 8.230187\n",
      "Total Batch finished: 30000, Accuracy: 0.005533, Loss: 8.156299\n",
      "Total Batch finished: 31000, Accuracy: 0.005548, Loss: 8.085939\n",
      "Total Batch finished: 32000, Accuracy: 0.005500, Loss: 8.018209\n",
      "Total Batch finished: 33000, Accuracy: 0.005545, Loss: 7.955000\n",
      "Total Batch finished: 34000, Accuracy: 0.005441, Loss: 7.896082\n",
      "Total Batch finished: 35000, Accuracy: 0.005514, Loss: 7.837329\n",
      "Total Batch finished: 36000, Accuracy: 0.005472, Loss: 7.783147\n",
      "Total Batch finished: 37000, Accuracy: 0.005514, Loss: 7.731010\n",
      "Total Batch finished: 38000, Accuracy: 0.005526, Loss: 7.681037\n",
      "Total Batch finished: 39000, Accuracy: 0.005462, Loss: 7.634207\n",
      "Total Batch finished: 40000, Accuracy: 0.005425, Loss: 7.590708\n",
      "Total Batch finished: 41000, Accuracy: 0.005390, Loss: 7.549065\n",
      "Total Batch finished: 42000, Accuracy: 0.005452, Loss: 7.507525\n",
      "Total Batch finished: 43000, Accuracy: 0.005465, Loss: 7.467775\n",
      "Total Batch finished: 44000, Accuracy: 0.005432, Loss: 7.431238\n",
      "Total Batch finished: 45000, Accuracy: 0.005400, Loss: 7.394967\n",
      "Total Batch finished: 46000, Accuracy: 0.005457, Loss: 7.359279\n",
      "Total Batch finished: 47000, Accuracy: 0.005532, Loss: 7.323089\n",
      "Total Batch finished: 48000, Accuracy: 0.005479, Loss: 7.290953\n",
      "Total Batch finished: 49000, Accuracy: 0.005571, Loss: 7.259121\n",
      "Total Batch finished: 50000, Accuracy: 0.005480, Loss: 7.228568\n",
      "Total Batch finished: 51000, Accuracy: 0.005510, Loss: 7.198820\n",
      "Total Batch finished: 52000, Accuracy: 0.005462, Loss: 7.170467\n",
      "Total Batch finished: 53000, Accuracy: 0.005528, Loss: 7.142642\n",
      "Total Batch finished: 54000, Accuracy: 0.005537, Loss: 7.115049\n",
      "Total Batch finished: 55000, Accuracy: 0.005509, Loss: 7.088733\n",
      "Total Batch finished: 56000, Accuracy: 0.005536, Loss: 7.063407\n",
      "Total Batch finished: 57000, Accuracy: 0.005491, Loss: 7.038866\n",
      "Total Batch finished: 58000, Accuracy: 0.005500, Loss: 7.015970\n",
      "Total Batch finished: 59000, Accuracy: 0.005475, Loss: 6.992377\n",
      "Total Batch finished: 60000, Accuracy: 0.005467, Loss: 6.970252\n",
      "Total Batch finished: 61000, Accuracy: 0.005541, Loss: 6.948554\n",
      "Total Batch finished: 62000, Accuracy: 0.005581, Loss: 6.927348\n",
      "Total Batch finished: 63000, Accuracy: 0.005603, Loss: 6.907696\n",
      "Total Batch finished: 64000, Accuracy: 0.005594, Loss: 6.888193\n",
      "Total Batch finished: 65000, Accuracy: 0.005600, Loss: 6.868575\n",
      "Total Batch finished: 66000, Accuracy: 0.005606, Loss: 6.848741\n",
      "Total Batch finished: 67000, Accuracy: 0.005642, Loss: 6.830331\n",
      "Total Batch finished: 68000, Accuracy: 0.005632, Loss: 6.812761\n",
      "Total Batch finished: 69000, Accuracy: 0.005696, Loss: 6.794712\n",
      "Total Batch finished: 70000, Accuracy: 0.005657, Loss: 6.777824\n",
      "Total Batch finished: 71000, Accuracy: 0.005620, Loss: 6.761264\n",
      "Total Batch finished: 72000, Accuracy: 0.005611, Loss: 6.745453\n",
      "Total Batch finished: 73000, Accuracy: 0.005658, Loss: 6.729362\n",
      "Total Batch finished: 74000, Accuracy: 0.005649, Loss: 6.713655\n",
      "Total Batch finished: 75000, Accuracy: 0.005640, Loss: 6.699107\n",
      "Total Batch finished: 76000, Accuracy: 0.005671, Loss: 6.684796\n",
      "Total Batch finished: 77000, Accuracy: 0.005662, Loss: 6.670151\n",
      "Total Batch finished: 78000, Accuracy: 0.005679, Loss: 6.655999\n",
      "Total Batch finished: 79000, Accuracy: 0.005684, Loss: 6.642192\n",
      "Total Batch finished: 80000, Accuracy: 0.005675, Loss: 6.628738\n",
      "Total Batch finished: 81000, Accuracy: 0.005667, Loss: 6.615854\n",
      "Total Batch finished: 82000, Accuracy: 0.005732, Loss: 6.602694\n",
      "Total Batch finished: 83000, Accuracy: 0.005807, Loss: 6.590241\n",
      "Total Batch finished: 84000, Accuracy: 0.005821, Loss: 6.578529\n",
      "Total Batch finished: 85000, Accuracy: 0.005894, Loss: 6.566489\n",
      "Total Batch finished: 86000, Accuracy: 0.005895, Loss: 6.554889\n",
      "Total Batch finished: 87000, Accuracy: 0.005885, Loss: 6.542937\n",
      "Total Batch finished: 88000, Accuracy: 0.005932, Loss: 6.530840\n",
      "Total Batch finished: 89000, Accuracy: 0.005899, Loss: 6.520085\n",
      "Total Batch finished: 90000, Accuracy: 0.005911, Loss: 6.508823\n",
      "Total Batch finished: 91000, Accuracy: 0.005934, Loss: 6.497966\n",
      "Total Batch finished: 92000, Accuracy: 0.005935, Loss: 6.487357\n",
      "Total Batch finished: 93000, Accuracy: 0.005914, Loss: 6.476631\n",
      "Total Batch finished: 94000, Accuracy: 0.005936, Loss: 6.466660\n",
      "Total Batch finished: 95000, Accuracy: 0.005916, Loss: 6.456585\n",
      "Total Batch finished: 96000, Accuracy: 0.005865, Loss: 6.446855\n",
      "Total Batch finished: 97000, Accuracy: 0.005845, Loss: 6.437176\n",
      "Total Batch finished: 98000, Accuracy: 0.005827, Loss: 6.427772\n",
      "Total Batch finished: 99000, Accuracy: 0.005838, Loss: 6.418340\n",
      "Total Batch finished: 100000, Accuracy: 0.005830, Loss: 6.409005\n",
      "(0, 'Train Accuracy:', 0.0058300000627059491, 'Train Loss:', 6.4090047311782836)\n",
      "Batching Validation...\n",
      "('Test Accuracy:', 0.0055000001098960642)\n",
      "Total Batch finished: 1000, Accuracy: 0.002000, Loss: 5.479743\n",
      "Total Batch finished: 2000, Accuracy: 0.003000, Loss: 5.490244\n",
      "Total Batch finished: 3000, Accuracy: 0.003667, Loss: 5.481567\n",
      "Total Batch finished: 4000, Accuracy: 0.004750, Loss: 5.490140\n",
      "Total Batch finished: 5000, Accuracy: 0.004800, Loss: 5.493230\n",
      "Total Batch finished: 6000, Accuracy: 0.005833, Loss: 5.483940\n",
      "Total Batch finished: 7000, Accuracy: 0.006000, Loss: 5.482946\n",
      "Total Batch finished: 8000, Accuracy: 0.006375, Loss: 5.488143\n",
      "Total Batch finished: 9000, Accuracy: 0.005889, Loss: 5.491115\n",
      "Total Batch finished: 10000, Accuracy: 0.005700, Loss: 5.489407\n",
      "Total Batch finished: 11000, Accuracy: 0.005818, Loss: 5.490407\n",
      "Total Batch finished: 12000, Accuracy: 0.005833, Loss: 5.488543\n",
      "Total Batch finished: 13000, Accuracy: 0.006154, Loss: 5.492821\n",
      "Total Batch finished: 14000, Accuracy: 0.006000, Loss: 5.494476\n",
      "Total Batch finished: 15000, Accuracy: 0.006000, Loss: 5.492944\n",
      "Total Batch finished: 16000, Accuracy: 0.006063, Loss: 5.489813\n",
      "Total Batch finished: 17000, Accuracy: 0.005941, Loss: 5.487576\n",
      "Total Batch finished: 18000, Accuracy: 0.005778, Loss: 5.489187\n",
      "Total Batch finished: 19000, Accuracy: 0.005895, Loss: 5.486287\n",
      "Total Batch finished: 20000, Accuracy: 0.006150, Loss: 5.483567\n",
      "Total Batch finished: 21000, Accuracy: 0.006190, Loss: 5.483275\n",
      "Total Batch finished: 22000, Accuracy: 0.006364, Loss: 5.482443\n",
      "Total Batch finished: 23000, Accuracy: 0.006217, Loss: 5.480974\n",
      "Total Batch finished: 24000, Accuracy: 0.006083, Loss: 5.481584\n",
      "Total Batch finished: 25000, Accuracy: 0.006040, Loss: 5.481026\n",
      "Total Batch finished: 26000, Accuracy: 0.006000, Loss: 5.479528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Batch finished: 27000, Accuracy: 0.006037, Loss: 5.477651\n",
      "Total Batch finished: 28000, Accuracy: 0.006071, Loss: 5.477294\n",
      "Total Batch finished: 29000, Accuracy: 0.006069, Loss: 5.477197\n",
      "Total Batch finished: 30000, Accuracy: 0.006067, Loss: 5.475989\n",
      "Total Batch finished: 31000, Accuracy: 0.006000, Loss: 5.474656\n",
      "Total Batch finished: 32000, Accuracy: 0.005938, Loss: 5.473199\n",
      "Total Batch finished: 33000, Accuracy: 0.005848, Loss: 5.472074\n",
      "Total Batch finished: 34000, Accuracy: 0.005765, Loss: 5.472054\n",
      "Total Batch finished: 35000, Accuracy: 0.005771, Loss: 5.470270\n",
      "Total Batch finished: 36000, Accuracy: 0.005750, Loss: 5.469306\n",
      "Total Batch finished: 37000, Accuracy: 0.005757, Loss: 5.468078\n",
      "Total Batch finished: 38000, Accuracy: 0.005816, Loss: 5.466606\n",
      "Total Batch finished: 39000, Accuracy: 0.005897, Loss: 5.465841\n",
      "Total Batch finished: 40000, Accuracy: 0.005850, Loss: 5.465690\n",
      "Total Batch finished: 41000, Accuracy: 0.005878, Loss: 5.465513\n",
      "Total Batch finished: 42000, Accuracy: 0.005786, Loss: 5.465071\n",
      "Total Batch finished: 43000, Accuracy: 0.005767, Loss: 5.464409\n",
      "Total Batch finished: 44000, Accuracy: 0.005750, Loss: 5.464672\n",
      "Total Batch finished: 45000, Accuracy: 0.005689, Loss: 5.464041\n",
      "Total Batch finished: 46000, Accuracy: 0.005783, Loss: 5.463069\n",
      "Total Batch finished: 47000, Accuracy: 0.005830, Loss: 5.461734\n",
      "Total Batch finished: 48000, Accuracy: 0.005854, Loss: 5.461289\n",
      "Total Batch finished: 49000, Accuracy: 0.005857, Loss: 5.460615\n",
      "Total Batch finished: 50000, Accuracy: 0.005840, Loss: 5.459773\n",
      "Total Batch finished: 51000, Accuracy: 0.005824, Loss: 5.459047\n",
      "Total Batch finished: 52000, Accuracy: 0.005827, Loss: 5.458554\n",
      "Total Batch finished: 53000, Accuracy: 0.005925, Loss: 5.457924\n",
      "Total Batch finished: 54000, Accuracy: 0.005963, Loss: 5.456963\n",
      "Total Batch finished: 55000, Accuracy: 0.005945, Loss: 5.456048\n",
      "Total Batch finished: 56000, Accuracy: 0.005982, Loss: 5.455355\n",
      "Total Batch finished: 57000, Accuracy: 0.005947, Loss: 5.454911\n",
      "Total Batch finished: 58000, Accuracy: 0.005931, Loss: 5.454813\n",
      "Total Batch finished: 59000, Accuracy: 0.005915, Loss: 5.454001\n",
      "Total Batch finished: 60000, Accuracy: 0.005933, Loss: 5.453435\n",
      "Total Batch finished: 61000, Accuracy: 0.006049, Loss: 5.452849\n",
      "Total Batch finished: 62000, Accuracy: 0.006097, Loss: 5.452340\n",
      "Total Batch finished: 63000, Accuracy: 0.006079, Loss: 5.452393\n",
      "Total Batch finished: 64000, Accuracy: 0.006047, Loss: 5.452193\n",
      "Total Batch finished: 65000, Accuracy: 0.006092, Loss: 5.451472\n",
      "Total Batch finished: 66000, Accuracy: 0.006076, Loss: 5.450446\n",
      "Total Batch finished: 67000, Accuracy: 0.006090, Loss: 5.450040\n",
      "Total Batch finished: 68000, Accuracy: 0.006029, Loss: 5.449599\n",
      "Total Batch finished: 69000, Accuracy: 0.006087, Loss: 5.448635\n",
      "Total Batch finished: 70000, Accuracy: 0.006071, Loss: 5.448152\n",
      "Total Batch finished: 71000, Accuracy: 0.006085, Loss: 5.447477\n",
      "Total Batch finished: 72000, Accuracy: 0.006111, Loss: 5.447162\n",
      "Total Batch finished: 73000, Accuracy: 0.006123, Loss: 5.446443\n",
      "Total Batch finished: 74000, Accuracy: 0.006122, Loss: 5.445727\n",
      "Total Batch finished: 75000, Accuracy: 0.006093, Loss: 5.445606\n",
      "Total Batch finished: 76000, Accuracy: 0.006118, Loss: 5.445412\n",
      "Total Batch finished: 77000, Accuracy: 0.006091, Loss: 5.444701\n",
      "Total Batch finished: 78000, Accuracy: 0.006077, Loss: 5.444073\n",
      "Total Batch finished: 79000, Accuracy: 0.006089, Loss: 5.443658\n",
      "Total Batch finished: 80000, Accuracy: 0.006125, Loss: 5.443084\n",
      "Total Batch finished: 81000, Accuracy: 0.006111, Loss: 5.442833\n",
      "Total Batch finished: 82000, Accuracy: 0.006122, Loss: 5.442205\n",
      "Total Batch finished: 83000, Accuracy: 0.006120, Loss: 5.441838\n",
      "Total Batch finished: 84000, Accuracy: 0.006119, Loss: 5.441751\n",
      "Total Batch finished: 85000, Accuracy: 0.006141, Loss: 5.441189\n",
      "Total Batch finished: 86000, Accuracy: 0.006151, Loss: 5.440955\n",
      "Total Batch finished: 87000, Accuracy: 0.006184, Loss: 5.440370\n",
      "Total Batch finished: 88000, Accuracy: 0.006205, Loss: 5.439500\n",
      "Total Batch finished: 89000, Accuracy: 0.006213, Loss: 5.439397\n",
      "Total Batch finished: 90000, Accuracy: 0.006200, Loss: 5.438621\n",
      "Total Batch finished: 91000, Accuracy: 0.006209, Loss: 5.438157\n",
      "Total Batch finished: 92000, Accuracy: 0.006207, Loss: 5.437645\n",
      "Total Batch finished: 93000, Accuracy: 0.006215, Loss: 5.437080\n",
      "Total Batch finished: 94000, Accuracy: 0.006223, Loss: 5.436754\n",
      "Total Batch finished: 95000, Accuracy: 0.006200, Loss: 5.436260\n",
      "Total Batch finished: 96000, Accuracy: 0.006208, Loss: 5.435833\n",
      "Total Batch finished: 97000, Accuracy: 0.006206, Loss: 5.435268\n",
      "Total Batch finished: 98000, Accuracy: 0.006194, Loss: 5.434889\n",
      "Total Batch finished: 99000, Accuracy: 0.006253, Loss: 5.434396\n",
      "Total Batch finished: 100000, Accuracy: 0.006260, Loss: 5.433777\n",
      "(1, 'Train Accuracy:', 0.0062600000668317076, 'Train Loss:', 5.4337768030166629)\n",
      "Batching Validation...\n",
      "('Test Accuracy:', 0.0056500001344829799)\n",
      "Total Batch finished: 1000, Accuracy: 0.001000, Loss: 5.381352\n",
      "Total Batch finished: 2000, Accuracy: 0.004500, Loss: 5.383162\n",
      "Total Batch finished: 3000, Accuracy: 0.004333, Loss: 5.376751\n",
      "Total Batch finished: 4000, Accuracy: 0.006500, Loss: 5.382575\n",
      "Total Batch finished: 5000, Accuracy: 0.006000, Loss: 5.387156\n",
      "Total Batch finished: 6000, Accuracy: 0.006000, Loss: 5.378285\n",
      "Total Batch finished: 7000, Accuracy: 0.006857, Loss: 5.377839\n",
      "Total Batch finished: 8000, Accuracy: 0.006625, Loss: 5.382937\n",
      "Total Batch finished: 9000, Accuracy: 0.006444, Loss: 5.385758\n",
      "Total Batch finished: 10000, Accuracy: 0.006300, Loss: 5.384106\n",
      "Total Batch finished: 11000, Accuracy: 0.006545, Loss: 5.384624\n",
      "Total Batch finished: 12000, Accuracy: 0.006583, Loss: 5.383422\n",
      "Total Batch finished: 13000, Accuracy: 0.006846, Loss: 5.386937\n",
      "Total Batch finished: 14000, Accuracy: 0.006786, Loss: 5.388535\n",
      "Total Batch finished: 15000, Accuracy: 0.006733, Loss: 5.387891\n",
      "Total Batch finished: 16000, Accuracy: 0.006813, Loss: 5.385736\n",
      "Total Batch finished: 17000, Accuracy: 0.006647, Loss: 5.385311\n",
      "Total Batch finished: 18000, Accuracy: 0.006611, Loss: 5.387084\n",
      "Total Batch finished: 19000, Accuracy: 0.006684, Loss: 5.385065\n",
      "Total Batch finished: 20000, Accuracy: 0.006700, Loss: 5.383726\n",
      "Total Batch finished: 21000, Accuracy: 0.006952, Loss: 5.383177\n",
      "Total Batch finished: 22000, Accuracy: 0.007136, Loss: 5.383243\n",
      "Total Batch finished: 23000, Accuracy: 0.007043, Loss: 5.382988\n",
      "Total Batch finished: 24000, Accuracy: 0.006917, Loss: 5.384209\n",
      "Total Batch finished: 25000, Accuracy: 0.006920, Loss: 5.384313\n",
      "Total Batch finished: 26000, Accuracy: 0.006962, Loss: 5.383778\n",
      "Total Batch finished: 27000, Accuracy: 0.006926, Loss: 5.382632\n",
      "Total Batch finished: 28000, Accuracy: 0.006929, Loss: 5.382756\n",
      "Total Batch finished: 29000, Accuracy: 0.006966, Loss: 5.383212\n",
      "Total Batch finished: 30000, Accuracy: 0.006933, Loss: 5.382529\n",
      "Total Batch finished: 31000, Accuracy: 0.006806, Loss: 5.381689\n",
      "Total Batch finished: 32000, Accuracy: 0.006969, Loss: 5.381146\n",
      "Total Batch finished: 33000, Accuracy: 0.006909, Loss: 5.380485\n",
      "Total Batch finished: 34000, Accuracy: 0.006824, Loss: 5.380837\n",
      "Total Batch finished: 35000, Accuracy: 0.006857, Loss: 5.379764\n",
      "Total Batch finished: 36000, Accuracy: 0.006861, Loss: 5.379232\n",
      "Total Batch finished: 37000, Accuracy: 0.006892, Loss: 5.378420\n",
      "Total Batch finished: 38000, Accuracy: 0.006921, Loss: 5.377401\n",
      "Total Batch finished: 39000, Accuracy: 0.007026, Loss: 5.377096\n",
      "Total Batch finished: 40000, Accuracy: 0.006925, Loss: 5.377070\n",
      "Total Batch finished: 41000, Accuracy: 0.006902, Loss: 5.377288\n",
      "Total Batch finished: 42000, Accuracy: 0.006786, Loss: 5.377301\n",
      "Total Batch finished: 43000, Accuracy: 0.006744, Loss: 5.377070\n",
      "Total Batch finished: 44000, Accuracy: 0.006705, Loss: 5.377648\n",
      "Total Batch finished: 45000, Accuracy: 0.006644, Loss: 5.377412\n",
      "Total Batch finished: 46000, Accuracy: 0.006696, Loss: 5.376958\n",
      "Total Batch finished: 47000, Accuracy: 0.006723, Loss: 5.376267\n",
      "Total Batch finished: 48000, Accuracy: 0.006729, Loss: 5.376096\n",
      "Total Batch finished: 49000, Accuracy: 0.006776, Loss: 5.375744\n",
      "Total Batch finished: 50000, Accuracy: 0.006800, Loss: 5.375226\n",
      "Total Batch finished: 51000, Accuracy: 0.006804, Loss: 5.374927\n",
      "Total Batch finished: 52000, Accuracy: 0.006788, Loss: 5.374801\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Batch finished: 53000, Accuracy: 0.006849, Loss: 5.374448\n",
      "Total Batch finished: 54000, Accuracy: 0.006907, Loss: 5.373982\n",
      "Total Batch finished: 55000, Accuracy: 0.006945, Loss: 5.373558\n",
      "Total Batch finished: 56000, Accuracy: 0.006982, Loss: 5.373340\n",
      "Total Batch finished: 57000, Accuracy: 0.006947, Loss: 5.373404\n",
      "Total Batch finished: 58000, Accuracy: 0.006897, Loss: 5.373503\n",
      "Total Batch finished: 59000, Accuracy: 0.006831, Loss: 5.373150\n",
      "Total Batch finished: 60000, Accuracy: 0.006817, Loss: 5.372953\n",
      "Total Batch finished: 61000, Accuracy: 0.006951, Loss: 5.372764\n",
      "Total Batch finished: 62000, Accuracy: 0.007016, Loss: 5.372609\n",
      "Total Batch finished: 63000, Accuracy: 0.007016, Loss: 5.372927\n",
      "Total Batch finished: 64000, Accuracy: 0.006938, Loss: 5.373056\n",
      "Total Batch finished: 65000, Accuracy: 0.007077, Loss: 5.372743\n",
      "Total Batch finished: 66000, Accuracy: 0.007030, Loss: 5.372263\n",
      "Total Batch finished: 67000, Accuracy: 0.007045, Loss: 5.372188\n",
      "Total Batch finished: 68000, Accuracy: 0.007015, Loss: 5.372039\n",
      "Total Batch finished: 69000, Accuracy: 0.007072, Loss: 5.371559\n",
      "Total Batch finished: 70000, Accuracy: 0.007057, Loss: 5.371421\n",
      "Total Batch finished: 71000, Accuracy: 0.007042, Loss: 5.371019\n",
      "Total Batch finished: 72000, Accuracy: 0.007083, Loss: 5.370976\n",
      "Total Batch finished: 73000, Accuracy: 0.007068, Loss: 5.370638\n",
      "Total Batch finished: 74000, Accuracy: 0.007081, Loss: 5.370270\n",
      "Total Batch finished: 75000, Accuracy: 0.007067, Loss: 5.370361\n",
      "Total Batch finished: 76000, Accuracy: 0.007132, Loss: 5.370399\n",
      "Total Batch finished: 77000, Accuracy: 0.007104, Loss: 5.369990\n",
      "Total Batch finished: 78000, Accuracy: 0.007128, Loss: 5.369643\n",
      "Total Batch finished: 79000, Accuracy: 0.007152, Loss: 5.369570\n",
      "Total Batch finished: 80000, Accuracy: 0.007175, Loss: 5.369215\n",
      "Total Batch finished: 81000, Accuracy: 0.007185, Loss: 5.369228\n",
      "Total Batch finished: 82000, Accuracy: 0.007171, Loss: 5.368905\n",
      "Total Batch finished: 83000, Accuracy: 0.007169, Loss: 5.368844\n",
      "Total Batch finished: 84000, Accuracy: 0.007155, Loss: 5.368921\n",
      "Total Batch finished: 85000, Accuracy: 0.007165, Loss: 5.368658\n",
      "Total Batch finished: 86000, Accuracy: 0.007140, Loss: 5.368707\n",
      "Total Batch finished: 87000, Accuracy: 0.007126, Loss: 5.368450\n",
      "Total Batch finished: 88000, Accuracy: 0.007114, Loss: 5.368028\n",
      "Total Batch finished: 89000, Accuracy: 0.007135, Loss: 5.368134\n",
      "Total Batch finished: 90000, Accuracy: 0.007111, Loss: 5.367625\n",
      "Total Batch finished: 91000, Accuracy: 0.007099, Loss: 5.367487\n",
      "Total Batch finished: 92000, Accuracy: 0.007163, Loss: 5.367218\n",
      "Total Batch finished: 93000, Accuracy: 0.007151, Loss: 5.366961\n",
      "Total Batch finished: 94000, Accuracy: 0.007128, Loss: 5.366835\n",
      "Total Batch finished: 95000, Accuracy: 0.007095, Loss: 5.366612\n",
      "Total Batch finished: 96000, Accuracy: 0.007094, Loss: 5.366422\n",
      "Total Batch finished: 97000, Accuracy: 0.007062, Loss: 5.366107\n",
      "Total Batch finished: 98000, Accuracy: 0.007061, Loss: 5.365975\n",
      "Total Batch finished: 99000, Accuracy: 0.007081, Loss: 5.365779\n",
      "Total Batch finished: 100000, Accuracy: 0.007080, Loss: 5.365417\n",
      "(2, 'Train Accuracy:', 0.0070800000603776425, 'Train Loss:', 5.3654170894622801)\n",
      "Batching Validation...\n",
      "('Test Accuracy:', 0.005833333466822902)\n",
      "Total Batch finished: 1000, Accuracy: 0.004000, Loss: 5.344156\n",
      "Total Batch finished: 2000, Accuracy: 0.006000, Loss: 5.342611\n",
      "Total Batch finished: 3000, Accuracy: 0.006333, Loss: 5.336905\n",
      "Total Batch finished: 4000, Accuracy: 0.008000, Loss: 5.343454\n",
      "Total Batch finished: 5000, Accuracy: 0.007400, Loss: 5.346905\n",
      "Total Batch finished: 6000, Accuracy: 0.007000, Loss: 5.338846\n",
      "Total Batch finished: 7000, Accuracy: 0.008000, Loss: 5.338219\n",
      "Total Batch finished: 8000, Accuracy: 0.007875, Loss: 5.343359\n",
      "Total Batch finished: 9000, Accuracy: 0.007444, Loss: 5.345622\n",
      "Total Batch finished: 10000, Accuracy: 0.007300, Loss: 5.344566\n",
      "Total Batch finished: 11000, Accuracy: 0.007364, Loss: 5.345000\n",
      "Total Batch finished: 12000, Accuracy: 0.007500, Loss: 5.344393\n",
      "Total Batch finished: 13000, Accuracy: 0.007462, Loss: 5.347059\n",
      "Total Batch finished: 14000, Accuracy: 0.007429, Loss: 5.348083\n",
      "Total Batch finished: 15000, Accuracy: 0.007467, Loss: 5.347557\n",
      "Total Batch finished: 16000, Accuracy: 0.007563, Loss: 5.345695\n",
      "Total Batch finished: 17000, Accuracy: 0.007529, Loss: 5.345538\n",
      "Total Batch finished: 18000, Accuracy: 0.007667, Loss: 5.347396\n",
      "Total Batch finished: 19000, Accuracy: 0.007737, Loss: 5.345883\n",
      "Total Batch finished: 20000, Accuracy: 0.007750, Loss: 5.344983\n",
      "Total Batch finished: 21000, Accuracy: 0.007857, Loss: 5.344225\n",
      "Total Batch finished: 22000, Accuracy: 0.007864, Loss: 5.344542\n",
      "Total Batch finished: 23000, Accuracy: 0.007739, Loss: 5.344402\n",
      "Total Batch finished: 24000, Accuracy: 0.007625, Loss: 5.345571\n",
      "Total Batch finished: 25000, Accuracy: 0.007520, Loss: 5.345869\n",
      "Total Batch finished: 26000, Accuracy: 0.007500, Loss: 5.345479\n",
      "Total Batch finished: 27000, Accuracy: 0.007333, Loss: 5.344711\n",
      "Total Batch finished: 28000, Accuracy: 0.007321, Loss: 5.344683\n",
      "Total Batch finished: 29000, Accuracy: 0.007276, Loss: 5.345299\n",
      "Total Batch finished: 30000, Accuracy: 0.007400, Loss: 5.344815\n",
      "Total Batch finished: 31000, Accuracy: 0.007290, Loss: 5.344115\n",
      "Total Batch finished: 32000, Accuracy: 0.007438, Loss: 5.343853\n",
      "Total Batch finished: 33000, Accuracy: 0.007455, Loss: 5.343338\n",
      "Total Batch finished: 34000, Accuracy: 0.007353, Loss: 5.343666\n",
      "Total Batch finished: 35000, Accuracy: 0.007371, Loss: 5.342838\n",
      "Total Batch finished: 36000, Accuracy: 0.007361, Loss: 5.342392\n",
      "Total Batch finished: 37000, Accuracy: 0.007378, Loss: 5.341659\n",
      "Total Batch finished: 38000, Accuracy: 0.007395, Loss: 5.340763\n",
      "Total Batch finished: 39000, Accuracy: 0.007513, Loss: 5.340484\n",
      "Total Batch finished: 40000, Accuracy: 0.007400, Loss: 5.340347\n",
      "Total Batch finished: 41000, Accuracy: 0.007366, Loss: 5.340597\n",
      "Total Batch finished: 42000, Accuracy: 0.007310, Loss: 5.340656\n",
      "Total Batch finished: 43000, Accuracy: 0.007302, Loss: 5.340494\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    trainAccuracy = []\n",
    "    trainLoss = []\n",
    "    valAccuracy = []\n",
    "    for epoch in range(n_epochs):       \n",
    "        # get the accuracy and loss, with the count\n",
    "        sumAccuracy = 0\n",
    "        sumLoss = 0\n",
    "        count = 0\n",
    "        \n",
    "        for batch in get_next_batch(1000):\n",
    "            X_batch, y_batch = batch[0], batch[1]\n",
    "            #print ('Training set', X_batch.shape, y_batch.shape)\n",
    "            _, myLoss = sess.run([training_op, loss], feed_dict={X: X_batch, y: y_batch})\n",
    "            sumAccuracy += accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            sumLoss += myLoss\n",
    "            count += 1\n",
    "            if (count * 1000 % 1000 == 0):\n",
    "                print(\"Total Batch finished: %d, Accuracy: %f, Loss: %f\" % (count * 1000, sumAccuracy/count, sumLoss/count))\n",
    "                \n",
    "            save_path = saver.save(sess, \"./tiny_imagenet/vgg_like\")  \n",
    "\n",
    "        trainAccuracy.append(sumAccuracy / count)\n",
    "        trainLoss.append(sumLoss / count)\n",
    "        print(epoch, \"Train Accuracy:\", trainAccuracy[-1], \"Train Loss:\", trainLoss[-1])\n",
    "\n",
    "        print(\"Batching Validation...\")\n",
    "        for batch in get_val_batch(1000):        \n",
    "            X_batch, y_batch = batch[0], batch[1]\n",
    "            valAccuracy.append(accuracy.eval(feed_dict={X: X_batch, y: y_batch}))\n",
    "        print(\"Test Accuracy:\", sum(valAccuracy)/len(valAccuracy))\n",
    "        \n",
    "    print(\"Final Train Accuracy: %f\" % (trainAccuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
